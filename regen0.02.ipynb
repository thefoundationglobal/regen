{"cells":[{"cell_type":"markdown","metadata":{"id":"CwFxs-2xP1bY"},"source":["<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"n-ox1sZ8P1be"},"source":["# Auto Generated Agent Chat: Performs Research with Multi-Agent Group Chat\n","regen Version 0.01, see [re-generative.ai](https://)\n","\n","**Instructions**\n","\n","make it your own:\n","1.  copy this notebook (by pressing \"File\" in the top right and then \"SaveCopy to Drive\")\n","\n","power it:\n","2. get a new OpenAI API key (you can get an Open-AI API key from ***platform***.openai.com, if you don't already have one - make sure that you have at least $1 to use it, check under \"Billig\")\n","3.  open \"Secrets\" from the sidebar (to the left with the key symbol),\n","4. press \"add a new secret\" and then enter \"openai-api\" under Name and your OpenAI API key (that you just got from platform.openai.com under\n"," \"Value\" button\n","5. toggle on/press \"Notebook Access\"\n","\n","enter your prompt:\n","6. press \"add a new secret\" again and enter your prompt  (your request to the LLM/AI)as a value beneath and name it \"prompt\" - make sure \"Notebook Access\" is toggled on/press for this one as well\n","\n","running the code:\n","4. press Ctrl + F9 and wait around half a minute (in German: Strg + F9)\n","\n","\n","scroll down in the notebook/main page and run the code blocks by hovering over the code fields and pressing the \"play\" button; make sure to wait until it says completed at the bottom - it should usually only need a few seconds each time. (feel free to read the explanations (and feel free to double check with us before - it is a branch of [this](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat/) code by an established company, in case you want to see the full documentation.))\n","5. see your results at the bottom - make sure to write \"continue\" (or whatever you think is appropriate) whenever the model asks you to\n","\n","Enjoy\n","\n","\n","**Notes**\n","\n","internal: link to previous [file](https://colab.research.google.com/drive/1b-mX7iEVh-RJGRUGgK8doj5gA90Gq8Zi?pli=1&authuser=2#scrollTo=4yKRjrfbP1bh)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWrPWbZwP1bf"},"outputs":[],"source":["#installing the model\n","%%capture --no-stderr\n","!pip install pyautogen~=0.2.0b4"]},{"cell_type":"markdown","metadata":{"id":"4yKRjrfbP1bh"},"source":["## Set your API Endpoint\n","\n","The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2jiFP5FP1bi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701368169547,"user_tz":-60,"elapsed":3701,"user":{"displayName":"Jakob Possert","userId":"03716471977077782914"}},"outputId":"c88051a7-09c1-4d38-e891-62b73f36df2e"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:The specified config_list file 'OAI_CONFIG_LIST' does not exist.\n"]}],"source":["import autogen\n","from google.colab import userdata\n","\n","config_list_gpt4 = autogen.config_list_from_json(\n","    \"OAI_CONFIG_LIST\",\n","    filter_dict={\n","        \"model\": [\"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n","    },\n",")"]},{"cell_type":"markdown","metadata":{"id":"1VPG2ZYFP1bi"},"source":["It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well).\n","\n","\n","You can set the value of config_list in any way you prefer. Please refer to this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/oai_openai_utils.ipynb) for full code examples of the different methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_JZvyjIP1bj"},"outputs":[],"source":["config_list = [\n","    {\n","        'model': 'gpt-4',\n","        'api_key': userdata.get('openai-api'),\n","    },\n","]\n","\n","\n","gpt4_config = {\n","    \"cache_seed\": 42,  # change the cache_seed for different trials\n","    \"temperature\": 0,\n","    \"config_list\": config_list,\n","    \"timeout\": 120,\n","}\n","user_proxy = autogen.UserProxyAgent(\n","   name=\"Admin\",\n","   system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin. Skip any expressions of gratititude.\",\n","   code_execution_config=False,\n",")\n","engineer = autogen.AssistantAgent(\n","    name=\"Engineer\",\n","    llm_config=gpt4_config,\n","    system_message='''Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n","Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor. Skip any expressions of gratititude.\n","If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n","''',\n",")\n","scientist = autogen.AssistantAgent(\n","    name=\"Scientist\",\n","    llm_config=gpt4_config,\n","    system_message=\"\"\"Scientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don't write code. Skip any expressions of gratititude.\"\"\"\n",")\n","planner = autogen.AssistantAgent(\n","    name=\"Planner\",\n","    system_message='''Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n","The plan may involve an engineer who can write code and a scientist who doesn't write code.\n","Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist. Skip any expressions of gratititude.\n","''',\n","    llm_config=gpt4_config,\n",")\n","executor = autogen.UserProxyAgent(\n","    name=\"Executor\",\n","    system_message=\"Executor. Execute the code written by the engineer and report the result. Remember that you can install packages in this environment by executing '!pip install <packagename>'. Skip any expressions of gratititude.\",\n","    human_input_mode=\"NEVER\",\n","    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"paper\"},\n",")\n","\n","observer = autogen.AssistantAgent(\n","    name=\"Observer\",\n","    system_message=\"Observer. Your role is to observe the dialogue and pause the dialogue if you think it would be beneficial for the user to get a chance to give more input. Skip any expressions of gratititude.\",\n","    human_input_mode= \"CONTINUE\",\n","    llm_config=gpt4_config,\n",")\n","\n","critic = autogen.AssistantAgent(\n","    name=\"Critic\",\n","    system_message=\"Critic. Double check plan, claims, code from other agents and provide feedback. Check whether the plan includes adding verifiable info such as source URL. Skip any expressions of gratititude.\",\n","    llm_config=gpt4_config,\n",")\n","groupchat = autogen.GroupChat(agents=[user_proxy, engineer, scientist, planner, executor, critic, observer], messages=[], max_round=50)\n","manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)"]},{"cell_type":"markdown","metadata":{"id":"E7kbIVnqP1bk"},"source":["## Start Chat"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vp7m9vBsP1bl","executionInfo":{"status":"error","timestamp":1701368640597,"user_tz":-60,"elapsed":322051,"user":{"displayName":"Jakob Possert","userId":"03716471977077782914"}},"outputId":"107c8d30-3e0d-44ab-f621-7dfc539b9850"},"outputs":[{"output_type":"stream","name":"stdout","text":["Admin (to chat_manager):\n","\n","how to affordably soundproof movable walls\n","\n","--------------------------------------------------------------------------------\n","Planner (to chat_manager):\n","\n","Plan:\n","\n","1. Scientist: Conduct a study to identify the most effective and affordable materials for soundproofing movable walls. This could include materials such as mass loaded vinyl, soundproof foam, or soundproof curtains.\n","\n","2. Scientist: Test these materials in a controlled environment to determine their effectiveness in soundproofing. This will involve creating a noise source and measuring the sound levels on both sides of the wall with and without the soundproofing material.\n","\n","3. Engineer: Based on the scientist's findings, design a prototype of a movable wall that incorporates the chosen soundproofing material. This design should take into account the need for the wall to be movable, so the material needs to be lightweight and flexible.\n","\n","4. Engineer: Build the prototype and test it in a real-world environment to see how well it performs. This could involve setting it up in a noisy office or home environment and measuring the sound levels on both sides of the wall.\n","\n","5. Scientist: Analyze the data from the real-world testing to determine if the prototype is effective at soundproofing. If it is not, return to step 1 and try a different material.\n","\n","6. Engineer: If the prototype is effective, finalize the design and create a plan for mass production. This will involve sourcing the materials, finding a manufacturer, and determining a price point.\n","\n","7. Engineer: Oversee the production process to ensure the walls are being made correctly and to the required standard.\n","\n","8. Scientist: Once the walls are produced, conduct a final round of testing to ensure they are effective at soundproofing.\n","\n","9. Engineer: If the final product passes testing, begin selling the soundproof movable walls to the public.\n","\n","--------------------------------------------------------------------------------\n","Critic (to chat_manager):\n","\n","The plan is comprehensive and covers all the necessary steps from research to production. However, it lacks verifiable information such as source URLs for the suggested materials like mass loaded vinyl, soundproof foam, or soundproof curtains. It would be beneficial to include references to where these materials can be purchased or more information about them can be found. \n","\n","Also, the plan assumes that the team has the resources to conduct scientific studies, design and build prototypes, and oversee mass production. It might be more realistic to research existing solutions, compare their effectiveness and cost, and then provide recommendations based on that research. \n","\n","Lastly, the plan doesn't include any steps for marketing or selling the final product. This is an important part of the process and should be included. \n","\n","Here's a revised plan:\n","\n","1. Researcher: Conduct a research to identify the most effective and affordable materials for soundproofing movable walls. This could include materials such as mass loaded vinyl, soundproof foam, or soundproof curtains. Include source URLs for these materials.\n","\n","2. Researcher: Compare the effectiveness and cost of these materials based on available data and customer reviews.\n","\n","3. Engineer: Based on the researcher's findings, design a prototype of a movable wall that incorporates the chosen soundproofing material. This design should take into account the need for the wall to be movable, so the material needs to be lightweight and flexible.\n","\n","4. Engineer: Build the prototype and test it in a real-world environment to see how well it performs. This could involve setting it up in a noisy office or home environment and measuring the sound levels on both sides of the wall.\n","\n","5. Researcher: Analyze the data from the real-world testing to determine if the prototype is effective at soundproofing. If it is not, return to step 1 and try a different material.\n","\n","6. Engineer: If the prototype is effective, finalize the design and create a plan for mass production. This will involve sourcing the materials, finding a manufacturer, and determining a price point.\n","\n","7. Engineer: Oversee the production process to ensure the walls are being made correctly and to the required standard.\n","\n","8. Researcher: Once the walls are produced, conduct a final round of testing to ensure they are effective at soundproofing.\n","\n","9. Marketer: Develop a marketing and sales strategy for the soundproof movable walls.\n","\n","10. Sales Team: Begin selling the soundproof movable walls to the public.\n","\n","--------------------------------------------------------------------------------\n","Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n","\n",">>>>>>>> NO HUMAN INPUT RECEIVED.\n","\n",">>>>>>>> USING AUTO REPLY...\n","Admin (to chat_manager):\n","\n","\n","\n","--------------------------------------------------------------------------------\n","Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n","\n",">>>>>>>> NO HUMAN INPUT RECEIVED.\n","\n",">>>>>>>> USING AUTO REPLY...\n","Admin (to chat_manager):\n","\n","\n","\n","--------------------------------------------------------------------------------\n","Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: continue\n","Admin (to chat_manager):\n","\n","continue\n","\n","--------------------------------------------------------------------------------\n","Scientist (to chat_manager):\n","\n","Scientist: To continue, the next step would be to gather feedback from the customers who have purchased and used the soundproof movable walls. This feedback can be used to make improvements to the product and to identify any issues that may have arisen during use. \n","\n","Engineer: Based on the feedback, make any necessary modifications to the design of the walls. This could involve changing the type of soundproofing material used, altering the way the walls are constructed, or making changes to the way the walls are installed.\n","\n","Scientist: Test the modified walls in a controlled environment to ensure that they still effectively block sound. If they do not, further modifications may be needed.\n","\n","Engineer: If the modified walls pass testing, update the production process to incorporate the changes. Oversee the production of a new batch of walls to ensure they are made correctly.\n","\n","Scientist: Conduct a final round of testing on the new batch of walls to ensure they are effective at soundproofing. If they pass this testing, they can be sold to the public.\n","\n","Marketer: Update the marketing and sales strategy to reflect any changes made to the product. This could involve highlighting new features or improvements, adjusting the price point, or targeting a different demographic of customers.\n","\n","Sales Team: Begin selling the updated soundproof movable walls to the public. Continue to gather feedback from customers to inform future improvements to the product.\n","\n","--------------------------------------------------------------------------------\n","Critic (to chat_manager):\n","\n","The continuation of the plan is logical and covers the necessary steps for product improvement based on customer feedback. However, it still lacks verifiable information such as source URLs for the suggested materials or methods. \n","\n","Also, the plan assumes that the team has the resources to conduct scientific studies, design and build prototypes, and oversee mass production. It might be more realistic to research existing solutions, compare their effectiveness and cost, and then provide recommendations based on that research. \n","\n","Here's a revised continuation:\n","\n","1. Researcher: Gather feedback from the customers who have purchased and used the soundproof movable walls. This feedback can be used to make improvements to the product and to identify any issues that may have arisen during use. \n","\n","2. Engineer: Based on the feedback, make any necessary modifications to the design of the walls. This could involve changing the type of soundproofing material used, altering the way the walls are constructed, or making changes to the way the walls are installed.\n","\n","3. Researcher: Test the modified walls in a real-world environment to ensure that they still effectively block sound. If they do not, further modifications may be needed.\n","\n","4. Engineer: If the modified walls pass testing, update the production process to incorporate the changes. Oversee the production of a new batch of walls to ensure they are made correctly.\n","\n","5. Researcher: Conduct a final round of testing on the new batch of walls to ensure they are effective at soundproofing. If they pass this testing, they can be sold to the public.\n","\n","6. Marketer: Update the marketing and sales strategy to reflect any changes made to the product. This could involve highlighting new features or improvements, adjusting the price point, or targeting a different demographic of customers.\n","\n","7. Sales Team: Begin selling the updated soundproof movable walls to the public. Continue to gather feedback from customers to inform future improvements to the product. \n","\n","Remember to document all changes, tests, and customer feedback with verifiable sources or data.\n","\n","--------------------------------------------------------------------------------\n","Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: continue\n","Admin (to chat_manager):\n","\n","continue\n","\n","--------------------------------------------------------------------------------\n","Scientist (to chat_manager):\n","\n","Scientist: To continue, the next step would be to monitor the sales and customer feedback of the updated soundproof movable walls. This will help to identify any recurring issues or areas for improvement.\n","\n","Engineer: Based on the feedback and sales data, make any necessary modifications to the design or production process of the walls. This could involve changing the type of soundproofing material used, altering the way the walls are constructed, or making changes to the way the walls are installed.\n","\n","Scientist: Test the newly modified walls in a controlled environment to ensure that they still effectively block sound. If they do not, further modifications may be needed.\n","\n","Engineer: If the modified walls pass testing, update the production process to incorporate the changes. Oversee the production of a new batch of walls to ensure they are made correctly.\n","\n","Scientist: Conduct a final round of testing on the new batch of walls to ensure they are effective at soundproofing. If they pass this testing, they can be sold to the public.\n","\n","Marketer: Update the marketing and sales strategy to reflect any changes made to the product. This could involve highlighting new features or improvements, adjusting the price point, or targeting a different demographic of customers.\n","\n","Sales Team: Begin selling the updated soundproof movable walls to the public. Continue to gather feedback from customers to inform future improvements to the product.\n","\n","--------------------------------------------------------------------------------\n","Critic (to chat_manager):\n","\n","The continuation of the plan is logical and covers the necessary steps for product improvement based on customer feedback and sales data. However, it still lacks verifiable information such as source URLs for the suggested materials or methods. \n","\n","Also, the plan assumes that the team has the resources to conduct scientific studies, design and build prototypes, and oversee mass production. It might be more realistic to research existing solutions, compare their effectiveness and cost, and then provide recommendations based on that research. \n","\n","Here's a revised continuation:\n","\n","1. Researcher: Monitor the sales and customer feedback of the updated soundproof movable walls. This will help to identify any recurring issues or areas for improvement. \n","\n","2. Engineer: Based on the feedback and sales data, make any necessary modifications to the design or production process of the walls. This could involve changing the type of soundproofing material used, altering the way the walls are constructed, or making changes to the way the walls are installed.\n","\n","3. Researcher: Test the newly modified walls in a real-world environment to ensure that they still effectively block sound. If they do not, further modifications may be needed.\n","\n","4. Engineer: If the modified walls pass testing, update the production process to incorporate the changes. Oversee the production of a new batch of walls to ensure they are made correctly.\n","\n","5. Researcher: Conduct a final round of testing on the new batch of walls to ensure they are effective at soundproofing. If they pass this testing, they can be sold to the public.\n","\n","6. Marketer: Update the marketing and sales strategy to reflect any changes made to the product. This could involve highlighting new features or improvements, adjusting the price point, or targeting a different demographic of customers.\n","\n","7. Sales Team: Begin selling the updated soundproof movable walls to the public. Continue to gather feedback from customers to inform future improvements to the product. \n","\n","Remember to document all changes, tests, and customer feedback with verifiable sources or data.\n","\n","--------------------------------------------------------------------------------\n","Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: continue\n","Admin (to chat_manager):\n","\n","continue\n","\n","--------------------------------------------------------------------------------\n"]},{"output_type":"error","ename":"RateLimitError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-4da7078d375b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m user_proxy.initiate_chat(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36minitiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \"\"\"\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_init_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     async def a_initiate_chat(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreply_at_receive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trigger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m                 \u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/groupchat.py\u001b[0m in \u001b[0;36mrun_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;31m# select the next speaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mspeaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_speaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                 \u001b[0;31m# let the speaker speak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/groupchat.py\u001b[0m in \u001b[0;36mselect_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# auto speaker selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_system_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_speaker_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         final, name = selector.generate_oai_reply(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             + [\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# TODO: #1143 handle token limit exceeded error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         response = client.create(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oai_system_message\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    245\u001b[0m                         \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# filter is not passed; try the next config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_completions_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAPIError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"config {i} failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/oai/client.py\u001b[0m in \u001b[0;36m_completions_create\u001b[0;34m(self, client, params)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         )\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 842\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    843\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    874\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    934\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    874\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    934\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-v7b1WD152YonexALNIkbfgyw on tokens per min (TPM): Limit 10000, Used 8689, Requested 3544. Please try again in 13.398s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"]}],"source":["user_proxy.initiate_chat(\n","    manager,\n","    message=userdata.get('prompt')\n",",\n",")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1b-mX7iEVh-RJGRUGgK8doj5gA90Gq8Zi","timestamp":1701366355732},{"file_id":"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_research.ipynb","timestamp":1700577724884}]},"kernelspec":{"display_name":"flaml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}